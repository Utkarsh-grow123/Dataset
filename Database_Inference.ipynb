{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "These results provide a detailed view of how well your logistic regression model distinguishes between the two classes (0 and 1) on your validation and test data. Here's what the numbers tell you about the experiment:\n",
        "\n",
        "## 1. **Overall Performance**\n",
        "\n",
        "- **Accuracy:**  \n",
        "  - Validation: ~91%  \n",
        "  - Test: ~85%  \n",
        "  The model correctly classifies approximately 91% of validation samples and 85% of test samples, which shows strong but slightly reduced performance on unseen test data—a common and expected result.\n",
        "\n",
        "- **Interpretation:**  \n",
        "  High accuracy on both sets indicates your model generally makes good predictions with modest performance drop on test data (which is normal).\n",
        "\n",
        "## 2. **Precision (Positive Predictive Value)**\n",
        "\n",
        "- Validation Class 0: 91%  \n",
        "- Validation Class 1: 90%  \n",
        "- Test Class 0: 83%  \n",
        "- Test Class 1: 87%  \n",
        "\n",
        "**Meaning:**  \n",
        "When the model predicts a sample as belonging to a class, it's correct 83–91% of the time depending on class and set. Precision is slightly better on validation, which suggests the model has learned well and generalizes fairly but also makes modestly more false positives on test data.\n",
        "\n",
        "## 3. **Recall (Sensitivity or True Positive Rate)**\n",
        "\n",
        "- Validation Class 0: 87%  \n",
        "- Validation Class 1: 93%  \n",
        "- Test Class 0: 83%  \n",
        "- Test Class 1: 87%  \n",
        "\n",
        "**Meaning:**  \n",
        "The model detects 83–93% of actual positives correctly. On validation, recall for Class 1 is higher (fewer false negatives), but it drops for Class 0 on test, indicating the model misses more true negatives or positives in test compared to validation.\n",
        "\n",
        "## 4. **F1-Score (Harmonic Mean of Precision and Recall)**\n",
        "\n",
        "- Validation Class 0: 89%  \n",
        "- Validation Class 1: 92%  \n",
        "- Test Class 0: 83%  \n",
        "- Test Class 1: 87%  \n",
        "\n",
        "**Meaning:**  \n",
        "Balances precision and recall. High values (>80%) mean your model balances false positives and false negatives reasonably well, with slightly stronger performance on validation.\n",
        "\n",
        "## 5. **ROC-AUC (Area under Receiver Operating Characteristic Curve)**\n",
        "\n",
        "- Validation: 0.9681  \n",
        "- Test: 0.9417  \n",
        "\n",
        "**Meaning:**  \n",
        "Highly discriminative model: 1.0 indicates perfect classification; values above 0.9 are considered excellent. It shows your model robustly distinguishes between the classes across thresholds.\n",
        "\n",
        "## 6. **PRC-AUC (Area under Precision-Recall Curve)**\n",
        "\n",
        "- Validation: 0.9748  \n",
        "- Test: 0.9606  \n",
        "\n",
        "**Meaning:**  \n",
        "Strong performance particularly for the positive class, excellent for imbalanced tasks: your model maintains high precision and recall across thresholds.\n",
        "\n",
        "## **Summary Interpretation**\n",
        "\n",
        "- **Strong classification model:** Logistic regression effectively separates classes with high precision and recall.  \n",
        "- **Generalizes well:** Slight performance drop on test compared to validation is expected but not dramatic.  \n",
        "- **Balanced errors:** Comparable precision and recall suggest no strong bias toward false positives or false negatives.  \n",
        "- **Potential improvements:** Less than perfect accuracy and recall on test suggests room for improvement, possibly via feature engineering, more complex models, or data augmentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "P3ni9QuTUSEb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_2kTuzwUKgR",
        "outputId": "f027b0c6-624b-4652-a577-546c23ea3269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation set metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.87      0.89        23\n",
            "           1       0.90      0.93      0.92        30\n",
            "\n",
            "    accuracy                           0.91        53\n",
            "   macro avg       0.91      0.90      0.90        53\n",
            "weighted avg       0.91      0.91      0.91        53\n",
            "\n",
            "Accuracy: 0.9057\n",
            "ROC-AUC: 0.9681\n",
            "PRC-AUC: 0.9748\n",
            "\n",
            "Test set metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83        24\n",
            "           1       0.87      0.87      0.87        30\n",
            "\n",
            "    accuracy                           0.85        54\n",
            "   macro avg       0.85      0.85      0.85        54\n",
            "weighted avg       0.85      0.85      0.85        54\n",
            "\n",
            "Accuracy: 0.8519\n",
            "ROC-AUC: 0.9417\n",
            "PRC-AUC: 0.9606\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, roc_auc_score,\n",
        "    precision_recall_curve, auc\n",
        ")\n",
        "\n",
        "# === Replace these with your actual raw GitHub URLs ===\n",
        "# # NON Member: 1\n",
        "# files_class1 = [\n",
        "#    X \"https://raw.githubusercontent.com/large-lang-model/Mech-MIA/main/data/csv/knowman-book.csv\",\n",
        "#     \"https://raw.githubusercontent.com/large-lang-model/Mech-MIA/main/data/csv/replica4.csv\"\n",
        "# ]\n",
        "# # MEMBER: 0\n",
        "# files_class0 = [\n",
        "#     \"https://raw.githubusercontent.com/large-lang-model/Mech-MIA/main/data/csv/worlds_facts.csv\",\n",
        "#   X  \"https://raw.githubusercontent.com/large-lang-model/Mech-MIA/main/data/csv/real_authers.csv\"\n",
        "# ]\n",
        "\n",
        "\n",
        "\n",
        "# NON Member: 1\n",
        "files_class1 = [\n",
        "\n",
        "    \"https://raw.githubusercontent.com/large-lang-model/Mech-MIA/main/data/csv/replica4.csv\"\n",
        "]\n",
        "# MEMBER: 0\n",
        "files_class0 = [\n",
        "    \"https://raw.githubusercontent.com/large-lang-model/Mech-MIA/main/data/csv/worlds_facts.csv\"\n",
        "\n",
        "]\n",
        "# ======================================================\n",
        "\n",
        "# Load and label each file accordingly\n",
        "dfs = []\n",
        "for file in files_class1:\n",
        "    df = pd.read_csv(\n",
        "        file,\n",
        "        engine='python',\n",
        "        quotechar='\"',\n",
        "        on_bad_lines='skip',\n",
        "        sep=',',\n",
        "        encoding='utf-8'\n",
        "    )\n",
        "    df[\"target\"] = 1\n",
        "    dfs.append(df)\n",
        "for file in files_class0:\n",
        "    df = pd.read_csv(\n",
        "        file,\n",
        "        engine='python',\n",
        "        quotechar='\"',\n",
        "        on_bad_lines='skip',\n",
        "        sep=',',\n",
        "        encoding='utf-8'\n",
        "    )\n",
        "    df[\"target\"] = 0\n",
        "    dfs.append(df)\n",
        "\n",
        "# Combine into one dataframe\n",
        "data = pd.concat(dfs).reset_index(drop=True)\n",
        "\n",
        "# Features: keep only numeric columns\n",
        "feature_cols = data.select_dtypes(include=['number']).columns.tolist()\n",
        "# Remove 'target' if it was included (shouldn't be with select_dtypes but as a safeguard)\n",
        "if 'target' in feature_cols:\n",
        "    feature_cols.remove('target')\n",
        "\n",
        "X = data[feature_cols]\n",
        "y = data[\"target\"]\n",
        "\n",
        "# Train/val/test split (60/20/20)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.4, stratify=y, random_state=42\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
        ")\n",
        "\n",
        "# Train logistic regression\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "def eval_metrics(y_true, preds, probs):\n",
        "    acc = accuracy_score(y_true, preds)\n",
        "    roc_auc = roc_auc_score(y_true, probs)\n",
        "    prec, recall, _ = precision_recall_curve(y_true, probs)\n",
        "    prc_auc = auc(recall, prec)\n",
        "    print(classification_report(y_true, preds))\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "    print(f\"PRC-AUC: {prc_auc:.4f}\")\n",
        "\n",
        "val_preds = clf.predict(X_val)\n",
        "val_probs = clf.predict_proba(X_val)[:, 1]\n",
        "print(\"\\nValidation set metrics:\")\n",
        "eval_metrics(y_val, val_preds, val_probs)\n",
        "\n",
        "test_preds = clf.predict(X_test)\n",
        "test_probs = clf.predict_proba(X_test)[:, 1]\n",
        "print(\"\\nTest set metrics:\")\n",
        "eval_metrics(y_test, test_preds, test_probs)"
      ]
    }
  ]
}